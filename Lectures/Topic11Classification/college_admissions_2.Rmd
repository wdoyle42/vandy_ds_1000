---
title: "Classification  via Logistic Regression"
author: "Will Doyle"
date: "`r Sys.Date()`"
editor_options: 
  markdown: 
    wrap: 80
---

```{r}
library(tidyverse)
library(tidymodels)
library(probably)
```


```{r}
ad<-read_rds("admit_data.rds")
```


## Logistic Regression

So far, we've been using tools we know for classification.

Logistic regression is set up to handle binary outcomes as the dependent
variable. The downside to logistic regression is that it is modeling the log
odds of the outcome, which means all of the coefficients are expressed as log
odds, which no one understands intuitively.

Let's take a look at a simple plot of our dependent variable as a function of one indpendent variable: SAT scores:


## Yield as a Function of SAT Scores
```{r}
ad%>%
  ggplot(aes(x=sat,y=yield))+
  geom_jitter(width=.01,height=.05,alpha=.25,color="blue")
```

We can see there are more higher SAT students than lower SAT students that ended up enrolling. A linear model in this case would look like this:

## Predicted "Probabilities" from a Linear Model
```{r}
ad%>%
  ggplot(aes(x=sat,y=yield))+
  geom_jitter(width=.01,height=.05,alpha=.25,color="blue")+
  geom_smooth(method="lm",se = FALSE,color="black")
```

We can see the issue we identified last time: we CAN fit a model, but it doesn't make a ton of sense. In particular, it doesn't follow the data very well and it ends up with probabilities outside 0,1. 

What we need is a better function that connects $y$  to $x$.  The idea of connecting $y$ to $x$ with a function other than a simple line is called a generalized linear model. 

A \textit{Generalized Linear Model} posits that $y$ is a function of the
independent variables and the coefficients or other parameters via a *link
function*:

$P(y|\mathbf{x})=G(\beta_0 + \mathbf{x_i\beta})$

In our case, we're interested in the probability that $y=1$

$P(y=1|\mathbf{x})=G(\beta_0 + \mathbf{x_i\beta})$

There are several functions that "map" onto a 0,1 continuum. The most
commonly used is the logistic function, which gives us the *logit model*.

The logistic function is given by:

$f(x)=\frac{1}{1+exp^{-k(x-x_0)}}$

## The Logistic Function: Pr(Y) as a Function of X
```{r}
x<-runif(100,-3,3)

pr_y=1/(1+exp(-x))

as_tibble(pr_y,x)%>%
  ggplot(aes(x=x,y=pr_y))+
  geom_smooth()

```


Mapped onto our GLM, this gives us:

$P(y=1|\mathbf{x})=\frac{exp(\beta_0 + \mathbf{x_i\beta})}{1+exp(\beta_0 +\mathbf{x_i\beta})}$

The critical thing to note about the above is that the link function maps the
entire result of estimation $(\beta_0 + \mathbf{x_i\beta})$ onto the 0,1
continuum. Thus, the change in the $P(y=1|\mathbf{x})$ is a function of *all* of
the independent variables and coefficients together, \textit{not} one at a time.

What does this mean? It means that the coefficients can only be interpreted on
the *logit* scale, and don't have the normal interpretation we would use
for OLS regression. Instead, to understand what the logistic regression
coefficients mean, you're going to have to convert the entire term
$(\beta_0 + \mathbf{x_i\beta})$ to the probability scale, using the inverse of
the function. Luckily we have computers to do this for us . . .

If we use this link function on our data, it would look like this:

## Plotting Predictions from Logistic Regression
```{r}
glm(yield ~ sat, family = binomial(link = "logit"), data = ad) %>%
  augment(type.predict = "response") %>%
  ggplot(aes(x = sat, y = yield)) +
  geom_jitter(
    width = .01,
    height = .05,
    alpha = .25,
    color = "blue"
  ) +
  geom_smooth(method = "lm", se = FALSE, color = "black")+
  geom_smooth(aes(x=sat,y=.fitted,color="red"))+
  theme(legend.position = "none")+
  ggtitle("Linear Prediction  in Black, Logistic in Red")

```


As you're getting started, this is what we recommend with these models:

- Use coefficient estimates for sign and significance only--don't try and come up with a substantive interpretationo
- Generate probability estimates based on characteristics for substantive interpretations.


## Using Tidymodels for Classification

For us to run a logistic regression in the tidymodels framework, the dependent variable has to be a factor. 

## Data Wrangling
```{r}
ad<-ad%>%
  mutate(yield_f=as_factor(ifelse(yield==1,"Yes","No")))
```


As usual, we're going to use the "tidymodels" approach to running this model, which works
much better for a standard data science workflow. It begins with splitting the
data into testing and training datasets using the `initial_split` function.

## Training and Testing
```{r}
split_data<-initial_split(ad)

ad_train<-training(split_data)

ad_test<-testing(split_data)

```


## Settting the Model

```{r}
logit_mod<-logistic_reg()%>%
  set_engine("glm")%>%
  set_mode("classification")
```


## Formula and Recipe
```{r}
admit_formula<-as.formula("yield_f~sat+net_price+legacy")

admit_recipe<-recipe(admit_formula,ad_train)
```


## Workflow
```{r}
ad_wf<-workflow()%>%
  add_model(logit_mod)%>%
  add_recipe(admit_recipe)
```

```{r}
ad_wf<-ad_wf%>%
  fit(ad_train)
```


```{r}
ad_wf%>%tidy()
```

```{r}
ad_wf%>%
  predict(ad_test)%>%
  bind_cols(ad_test)%>%
 sens(truth=yield_f,estimate=.pred_class,event_level="second")
```

```{r}
ad_wf%>%
  predict(ad_test)%>%
  bind_cols(ad_test)%>%
 spec(truth=yield_f,estimate=.pred_class,event_level="second")
```

```{r}
ad_wf%>%
  predict(ad_test)%>%
  bind_cols(ad_test)%>%
 accuracy(truth=yield_f,estimate=.pred_class,event_level="second")
```

```{r}

threshold_example<-ad_wf%>%
  predict(ad_test,type="prob")%>%
  bind_cols(ad_test)%>%
   threshold_perf(truth=yield_f,
                 estimate=.pred_Yes,
                 thresholds=seq(0,1,by=.1),metrics=c("sens","spec"))

ggplot(filter(threshold_example,.metric%in%c("sens","spec")),
       aes(x=.threshold,y=.estimate,color=.metric))+
  geom_line()
  
```

```{r}
ad_wf%>%
  predict(ad_test,type="prob")%>%
  bind_cols(ad_test)%>%
  roc_auc(truth=yield_f,estimate=.pred_Yes,event_level="second")
```

```{r}
ad_wf%>%
  predict(ad_test,type="prob")%>%
  bind_cols(ad_test)%>%
  roc_curve(truth=yield_f,.estimate=.pred_Yes,event_level="second")%>%
  autoplot()
```



